// Note: This code assumes the existence of external functions like power,
// zero3, randlc, vranlc, bubble, and comm3, and potentially global
// variables like is1, is2, is3, ie1, ie2, ie3, which are common in
// benchmark codes like NPB where this function originates.
// Necessary header includes are added for standard library functions used.

#include <stdio.h>
#include <math.h> // For pow

// External function declarations (assuming these functions are defined elsewhere)
// Their signatures are inferred from usage in the original code.
double power(double a, int n);
void zero3(double ***z, int n1, int n2, int n3);
double randlc(double *x, double a);
void vranlc(int n, double *x, double a, double y[]);
// Assuming MM is defined before bubble is used if its size isn't fixed
void bubble(double ten[][2], int j1[][2], int j2[][2], int j3[][2], int m, int ind);
void comm3(double ***z, int n1, int n2, int n3, int k);

// Assuming global variables used for array indexing and dimensions exist elsewhere
extern int is1, is2, is3, ie1, ie2, ie3;

static void zran3(double ***z, int n1, int n2, int n3, int nx, int ny, int k) {
#define MM	10 // Maxima/Minima count
#define	A	pow(5.0,13) // RNG parameter
#define	X	314159265.e0 // Initial RNG seed (double literal)

    // Declare variables used throughout the function
    int i0, m0, m1;
    int i1, i2, i3, d1, e1, e2, e3;
    double xx, x0, x1, a1, a2, ai;
    double ten[MM][2], best; // Stores top MM max ([:,1]) and bottom MM min ([:,0]) values
    int i, j1[MM][2], j2[MM][2], j3[MM][2]; // Stores indices (i1, i2, i3) corresponding to values in ten
    int jg[4][MM][2]; // Final global indices of top/bottom MM values
    double rdummy; // Dummy variable for unused return values

    //---------------------------------------------------------------------
    // Phase 0: Setup and Initial Zeroing of Z Array
    // This phase performs initial calculations and clears the z array.
    // The zeroing loop nest is independent across iterations and suitable for parallelization.
    // The scalar calculations before zeroing are sequential setup.
    //---------------------------------------------------------------------
    a1 = power( A, nx );
    a2 = power( A, nx*ny );

    // Initializing z to zero. This loop nest is independent across dimensions.
    // Potentially parallelizable using OpenMP parallel for on one or more loops.
    zero3(z,n1,n2,n3);

    // Calculate initial index based on global start indices (is1, is2, is3) and dimensions (nx, ny)
    i = is1 - 1 + nx * (is2 - 1 + ny * (is3 - 1));
    // Calculate power of A based on the initial index for RNG initialization
    ai = power( A, i );

    // Calculate dimensions for random number generation loop bounds
    d1 = ie1 - is1 + 1; // Size of the innermost dimension chunk to generate
    e1 = ie1 - is1 + 2; // Loop upper bound (exclusive) for i1 in the original code (starts from 1)
    e2 = ie2 - is2 + 2; // Loop upper bound (exclusive) for i2 (starts from 1)
    e3 = ie3 - is3 + 2; // Loop upper bound (exclusive) for i3 (starts from 1)

    //---------------------------------------------------------------------
    // Phase 1: Generate Random Numbers and Fill Array Z
    // Fills the array z with pseudo-random numbers generated by randlc and vranlc.
    // This phase exhibits significant loop-carried dependencies due to the iterative
    // updates of the random number generator state variables (seeds x0, x1, xx).
    // Parallelizing this section efficiently requires a parallel random number
    // generation strategy, which is not directly achievable with standard OpenMP
    // directives on these loops without modifying the RNG algorithm or seed management.
    // As written, the dependency chain of seeds makes the nested loops largely sequential.
    //---------------------------------------------------------------------
    x0 = X; // Initial seed for the outermost loop
    rdummy = randlc( &x0, ai ); // First seed update before the loops

    // Outer loop over i3. x0 carries dependency across i3 iterations.
    for (i3 = 1; i3 < e3; i3++) { // Potential parallel loop if seeds pre-calculated/managed
        x1 = x0; // Seed for inner loop is the state of x0 from previous outer iteration
        // Middle loop over i2. x1 carries dependency across i2 iterations within i3.
	for (i2 = 1; i2 < e2; i2++) { // Potential parallel loop if seeds pre-calculated/managed
            xx = x1; // Seed for vranlc is the state of x1 from previous middle iteration
            // Generate d1 random numbers starting with seed xx and store them in z[i3][i2][0...d1-1].
            // vranlc updates xx internally. This is a sequential dependency within the call.
            vranlc( d1, &xx, A, &(z[i3][i2][0])); // Sequential computation and update of xx
            rdummy = randlc( &x1, a1 ); // Update seed x1 based on its previous state
	}
	rdummy = randlc( &x0, a2 ); // Update seed x0 based on its state at the end of the i2 loop
    }

    //---------------------------------------------------------------------
    // Phase 2: Initialize Temporary Arrays for Min/Max Tracking
    // Initializes the 'ten' array (to hold top MM max and bottom MM min values)
    // and the 'j*' arrays (to hold their corresponding indices).
    // This loop is independent across iterations.
    // Potentially parallelizable, but MM is small (10), so parallel overhead
    // would likely exceed benefits for a simple parallel for.
    //---------------------------------------------------------------------
    for (i = 0; i < MM; i++) {
        // Initialize max tracking (column 1): values to 0.0, indices to 0
	ten[i][1] = 0.0;  j1[i][1] = 0; j2[i][1] = 0; j3[i][1] = 0;
        // Initialize min tracking (column 0): values to 1.0, indices to 0
	ten[i][0] = 1.0;  j1[i][0] = 0; j2[i][0] = 0; j3[i][0] = 0;
    }

    //---------------------------------------------------------------------
    // Phase 3: Find Top MM Maximum and Bottom MM Minimum Values in Z
    // Iterates through the interior of the z array (excluding boundaries).
    // For each element, it checks if it's larger than the current smallest
    // of the top MM max values (ten[0][1]) or smaller than the current largest
    // of the bottom MM min values (ten[0][0]). If so, it updates the value
    // and indices and calls 'bubble' to maintain the sorted lists of top/bottom MM.
    // This phase is a major sequential bottleneck for parallel execution.
    // The updates to the shared 'ten' and 'j*' arrays within the inner loop
    // body create loop-carried dependencies and significant race conditions
    // if parallelized naively. The 'bubble' function also operates on this
    // shared state. Efficient parallelization requires a different algorithm,
    // such as having each thread find local top/bottom MM lists and then merging
    // these lists in a separate step (a form of parallel reduction).
    // As written, applying OpenMP would likely require strong synchronization
    // (e.g., critical sections) around the 'if' blocks and 'bubble' calls,
    // effectively serializing the core work.
    //---------------------------------------------------------------------
    // Loop over the interior of the z array
    for (i3 = 1; i3 < n3-1; i3++) { // Loop iterations are potentially independent if inner part is synchronized/handled
	for (i2 = 1; i2 < n2-1; i2++) { // Loop iterations are potentially independent if inner part is synchronized/handled
            for (i1 = 1; i1 < n1-1; i1++) { // Loop iterations are potentially independent if inner part is synchronized/handled
                // The operations inside this innermost loop access and modify shared
                // state (ten, j* arrays). Parallelizing this loop nest efficiently
                // requires either a completely different algorithmic approach (e.g.,
                // local min/max tracking and merging) or explicit synchronization
                // which will limit parallelism severely.
		if ( z[i3][i2][i1] > ten[0][1] ) {
		    ten[0][1] = z[i3][i2][i1];
		    j1[0][1] = i1; j2[0][1] = i2; j3[0][1] = i3;
		    bubble( ten, j1, j2, j3, MM, 1 ); // Modifies shared arrays, requires synchronization
		}
		if ( z[i3][i2][i1] < ten[0][0] ) {
		    ten[0][0] = z[i3][i2][i1];
		    j1[0][0] = i1; j2[0][0] = i2; j3[0][0] = i3;
		    bubble( ten, j1, j2, j3, MM, 0 ); // Modifies shared arrays, requires synchronization
		}
	    }
	}
    }

    //---------------------------------------------------------------------
    // Phase 4: Store Final Results (Top/Bottom MM Indices and Values)
    // Uses the indices (j*) found in Phase 3 to locate the corresponding values
    // in z (stored in 'best') and populate the 'jg' array with global indices.
    // It also finalizes the 'ten' array.
    // This phase has loop-carried dependencies on the indices i0 and i1, which
    // are used to access the j* arrays and are decremented conditionally within
    // the loop. Accessing j* based on i0/i1 and updating i0/i1 makes the loop
    // sequential or requires careful management of these shared indices (e.g., atomics).
    //---------------------------------------------------------------------
    i1 = MM - 1; // Initial index for accessing the sorted max results in j*
    i0 = MM - 1; // Initial index for accessing the sorted min results in j*
    // Loop iterates backward from MM-1 to 0
    for (i = MM - 1 ; i >= 0; i--) { // This loop is sequential due to dependencies on i0 and i1
        // Access z using indices from j* based on the dependent index i1
	best = z[j3[i1][1]][j2[i1][1]][j1[i1][1]];
        // The condition 'best == z[...]' right after 'best' is assigned from z[...]
        // appears likely to be always true in practice for non-NaN values,
        // assuming standard floating point behavior and no precision issues.
        // If this condition is indeed always true, the indices i1 and i0 are
        // simply decremented every iteration, creating a simple loop-carried dependency.
        // If the condition can be false, the dependency structure is conditional.
        // Assuming the common interpretation where i0/i1 are always decremented:
        // if (best == z[j3[i1][1]][j2[i1][1]][j1[i1][1]]) {
            jg[0][i][1] = 0; // Populate jg array at index i
            jg[1][i][1] = is1 - 1 + j1[i1][1]; // Calculate and store global index
            jg[2][i][1] = is2 - 1 + j2[i1][1];
            jg[3][i][1] = is3 - 1 + j3[i1][1];
            i1 = i1-1; // Loop-carried dependency: i1 is updated and used in the next iteration
	// } else { ... }
	ten[i][1] = best; // Finalize ten array at index i

        // Access z using indices from j* based on the dependent index i0
	best = z[j3[i0][0]][j2[i0][0]][j1[i0][0]];
        // Assuming the condition is effectively always true:
        // if (best == z[j3[i0][0]][j2[i0][0]][j1[i0][0]]) {
            jg[0][i][0] = 0; // Populate jg array at index i
            jg[1][i][0] = is1 - 1 + j1[i0][0]; // Calculate and store global index
            jg[2][i][0] = is2 - 1 + j2[i0][0];
            jg[3][i][0] = is3 - 1 + j3[i0][0];
            i0 = i0-1; // Loop-carried dependency: i0 is updated and used in the next iteration
	// } else { ... }
	ten[i][0] = best; // Finalize ten array at index i
    }

    // Final values of i0 and i1 determine the range for subsequent phases
    m1 = i1 + 1; // Start index for maximum values in j* that were used
    m0 = i0 + 1; // Start index for minimum values in j* that were used

    //---------------------------------------------------------------------
    // Phase 5: Re-initialize the Entire Z Array to Zero
    // Clears the z array again before setting specific elements based on min/max results.
    // This loop nest is independent across iterations.
    // Can be parallelized using OpenMP parallel for on one or more loops.
    //---------------------------------------------------------------------
    for (i3 = 0; i3 < n3; i3++) { // Independent outer loop
	for (i2 = 0; i2 < n2; i2++) { // Independent middle loop
            for (i1 = 0; i1 < n1; i1++) { // Independent inner loop
		z[i3][i2][i1] = 0.0;
	    }
	}
    }

    //---------------------------------------------------------------------
    // Phase 6: Set Specific Elements in Z to -1.0 or 1.0
    // Uses the indices stored in j* from Phase 3/4 to mark specific points
    // in the z array corresponding to the identified min and max values.
    // Each iteration writes to a unique location in z determined by the indices
    // from j*. These writes are independent across iterations *unless* there
    // are duplicate min/max values resulting in the same index triplet (j3, j2, j1)
    // being stored multiple times in the relevant range of j*. Assuming indices
    // are unique or collisions are handled (e.g., only the last write matters),
    // these loops are potentially parallelizable.
    //---------------------------------------------------------------------
    // Loop to set elements corresponding to the bottom m0 minimum values to -1.0
    // The loop range is determined by m0 calculated in Phase 4.
    for (i = MM-1; i >= m0; i--) { // Potentially parallel loop
        // Write to z at indices read from j*[i][0].
        // Independent write operations if indices (j3[i][0], j2[i][0], j1[i][0]) are unique for each i in the range.
	z[j3[i][0]][j2[i][0]][j1[i][0]] = -1.0;
    }
    // Loop to set elements corresponding to the top m1 maximum values to 1.0
    // The loop range is determined by m1 calculated in Phase 4.
    for (i = MM-1; i >= m1; i--) { // Potentially parallel loop
        // Write to z at indices read from j*[i][1].
        // Independent write operations if indices (j3[i][1], j2[i][1], j1[i][1]) are unique for each i in the range.
	z[j3[i][1]][j2[i][1]][j1[i][1]] = 1.0;
    }

    //---------------------------------------------------------------------
    // Phase 7: Communication or Boundary Exchange
    // This is an external function call, likely responsible for handling
    // boundary conditions or inter-process communication required by the
    // overall benchmark (e.g., in a distributed memory context).
    // Parallelization depends entirely on the implementation of comm3.
    // This function itself cannot be modified or parallelized here.
    //---------------------------------------------------------------------
    comm3(z,n1,n2,n3,k);

    // Optional: Undefine macros if they are not intended to be global
    // #undef MM
    // #undef A
    // #undef X
}